{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26f0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d34d02",
   "metadata": {},
   "source": [
    "## Importing the CPP data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef7a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb3040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e242b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.clang-format',\n",
       " '.clang-tidy',\n",
       " '.devops',\n",
       " '.dockerignore',\n",
       " '.ecrc',\n",
       " '.editorconfig',\n",
       " '.flake8',\n",
       " '.git',\n",
       " '.github',\n",
       " '.gitignore']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check that the repo exists in your current working directory\n",
    "os.listdir(\"llama.cpp\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be902299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 287 .cpp files\n",
      "Example files:\n",
      " ['llama.cpp\\\\common\\\\arg.cpp', 'llama.cpp\\\\common\\\\chat-parser.cpp', 'llama.cpp\\\\common\\\\chat.cpp', 'llama.cpp\\\\common\\\\common.cpp', 'llama.cpp\\\\common\\\\console.cpp', 'llama.cpp\\\\common\\\\json-partial.cpp', 'llama.cpp\\\\common\\\\json-schema-to-grammar.cpp', 'llama.cpp\\\\common\\\\llguidance.cpp', 'llama.cpp\\\\common\\\\log.cpp', 'llama.cpp\\\\common\\\\ngram-cache.cpp']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all .cpp files inside the cloned repo (recursively)\n",
    "cpp_files = glob.glob(\"llama.cpp/**/*.cpp\", recursive=True)\n",
    "\n",
    "print(f\"‚úÖ Found {len(cpp_files)} .cpp files\")\n",
    "print(\"Example files:\\n\", cpp_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c631b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping (no permission): llama.cpp\\tools\\run\\linenoise.cpp\n"
     ]
    }
   ],
   "source": [
    "combined_cpp_text = \"\"\n",
    "\n",
    "for file in cpp_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            combined_cpp_text += f.read() + \"\\n\\n\"\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Skipping (no permission): {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b831613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9630781"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_cpp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72490de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1921171\n",
      "Unique tokens: 43243\n",
      "\n",
      "Top 20 most frequent tokens:\n",
      "\n",
      "token  frequency\n",
      "    ,     158108\n",
      "    )     133173\n",
      "    (     133149\n",
      "    ;      97125\n",
      "    =      75513\n",
      "    .      55306\n",
      "    -      52459\n",
      "    {      48510\n",
      "    }      48324\n",
      "    /      47706\n",
      "    :      46015\n",
      "    *      38322\n",
      "    >      36900\n",
      "    [      27829\n",
      "    ]      27537\n",
      "    <      22904\n",
      "const      20527\n",
      "    +      19941\n",
      "    &      15324\n",
      "   if      11989\n",
      "üìÅ Saved token frequencies to cpp_vocabulary.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ‚úÖ Use your existing in-memory string\n",
    "cpp_text = combined_cpp_text  # already loaded earlier\n",
    "\n",
    "# ‚úÖ Tokenize (C++ syntax-aware)\n",
    "tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', cpp_text)\n",
    "\n",
    "# ‚úÖ Count token frequencies\n",
    "token_counts = Counter(tokens)\n",
    "df_counts = pd.DataFrame(token_counts.items(), columns=[\"token\", \"frequency\"])\n",
    "df_counts = df_counts.sort_values(by=\"frequency\", ascending=False)\n",
    "\n",
    "# ‚úÖ Show stats\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Unique tokens:\", len(df_counts))\n",
    "\n",
    "print(\"\\nTop 20 most frequent tokens:\\n\")\n",
    "print(df_counts.head(20).to_string(index=False))\n",
    "\n",
    "# ‚úÖ Optionally save vocabulary to CSV for later reuse\n",
    "df_counts.to_csv(\"cpp_vocabulary.csv\", index=False)\n",
    "print(\"üìÅ Saved token frequencies to cpp_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f4d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 1,921,166 training samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ‚úÖ Create mappings (string ‚Üî integer)\n",
    "stoi = {s: i for i, s in enumerate(df_counts[\"token\"])}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "# ‚úÖ Define context size (how many previous tokens to use for prediction)\n",
    "context_size = 5  # you can experiment with 3‚Äì10\n",
    "\n",
    "# ‚úÖ Prepare X (context) and Y (target) lists\n",
    "X, Y = [], []\n",
    "\n",
    "for i in range(len(tokens) - context_size):\n",
    "    context = tokens[i:i + context_size]\n",
    "    target = tokens[i + context_size]\n",
    "    X.append([stoi[t] for t in context])\n",
    "    Y.append(stoi[target])\n",
    "\n",
    "print(f\"‚úÖ Created {len(X):,} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214bca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1921166, 5])\n",
      "Y shape: torch.Size([1921166])\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Move to torch tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33187b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a639d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 8158.2410\n",
      "Epoch 2/100, Loss: 5987.7782\n",
      "Epoch 3/100, Loss: 4995.5158\n",
      "Epoch 4/100, Loss: 4347.9852\n",
      "Epoch 5/100, Loss: 3900.1983\n",
      "Epoch 6/100, Loss: 3581.9716\n",
      "Epoch 7/100, Loss: 3349.8313\n",
      "Epoch 8/100, Loss: 3166.2782\n",
      "Epoch 9/100, Loss: 3023.9326\n",
      "Epoch 10/100, Loss: 2899.5545\n",
      "Epoch 11/100, Loss: 2790.9057\n",
      "Epoch 12/100, Loss: 2696.9978\n",
      "Epoch 13/100, Loss: 2610.9597\n",
      "Epoch 14/100, Loss: 2535.2656\n",
      "Epoch 15/100, Loss: 2465.0388\n",
      "Epoch 16/100, Loss: 2399.4459\n",
      "Epoch 17/100, Loss: 2340.3272\n",
      "Epoch 18/100, Loss: 2286.2762\n",
      "Epoch 19/100, Loss: 2235.7326\n",
      "Epoch 20/100, Loss: 2187.9197\n",
      "Epoch 21/100, Loss: 2144.1381\n",
      "Epoch 22/100, Loss: 2101.4902\n",
      "Epoch 23/100, Loss: 2062.9437\n",
      "Epoch 24/100, Loss: 2026.0425\n",
      "Epoch 25/100, Loss: 1991.7497\n",
      "Epoch 26/100, Loss: 1958.1768\n",
      "Epoch 27/100, Loss: 1927.1082\n",
      "Epoch 28/100, Loss: 1898.3764\n",
      "Epoch 29/100, Loss: 1871.8807\n",
      "Epoch 30/100, Loss: 1845.8584\n",
      "Epoch 31/100, Loss: 1821.3546\n",
      "Epoch 32/100, Loss: 1798.3707\n",
      "Epoch 33/100, Loss: 1776.7507\n",
      "Epoch 34/100, Loss: 1755.7106\n",
      "Epoch 35/100, Loss: 1736.1309\n",
      "Epoch 36/100, Loss: 1718.0862\n",
      "Epoch 37/100, Loss: 1701.0948\n",
      "Epoch 38/100, Loss: 1684.7341\n",
      "Epoch 39/100, Loss: 1668.1499\n",
      "Epoch 40/100, Loss: 1653.3345\n",
      "Epoch 41/100, Loss: 1638.9874\n",
      "Epoch 42/100, Loss: 1625.5924\n",
      "Epoch 43/100, Loss: 1612.6333\n",
      "Epoch 44/100, Loss: 1600.3539\n",
      "Epoch 45/100, Loss: 1588.8766\n",
      "Epoch 46/100, Loss: 1576.8363\n",
      "Epoch 47/100, Loss: 1567.0680\n",
      "Epoch 48/100, Loss: 1557.1219\n",
      "Epoch 49/100, Loss: 1547.7156\n",
      "Epoch 50/100, Loss: 1537.7512\n",
      "Epoch 51/100, Loss: 1530.5579\n",
      "Epoch 52/100, Loss: 1520.7424\n",
      "Epoch 53/100, Loss: 1512.6705\n",
      "Epoch 54/100, Loss: 1505.0107\n",
      "Epoch 55/100, Loss: 1497.5772\n",
      "Epoch 56/100, Loss: 1490.6193\n",
      "Epoch 57/100, Loss: 1483.7742\n",
      "Epoch 58/100, Loss: 1476.9077\n",
      "Epoch 59/100, Loss: 1470.8612\n",
      "Epoch 60/100, Loss: 1464.7945\n",
      "Epoch 61/100, Loss: 1458.6326\n",
      "Epoch 62/100, Loss: 1453.5203\n",
      "Epoch 63/100, Loss: 1448.2998\n",
      "Epoch 64/100, Loss: 1442.3232\n",
      "Epoch 65/100, Loss: 1437.7400\n",
      "Epoch 66/100, Loss: 1432.5934\n",
      "Epoch 67/100, Loss: 1427.5979\n",
      "Epoch 68/100, Loss: 1422.9235\n",
      "Epoch 69/100, Loss: 1418.9552\n",
      "Epoch 70/100, Loss: 1414.1195\n",
      "Epoch 71/100, Loss: 1409.9691\n",
      "Epoch 72/100, Loss: 1405.7970\n",
      "Epoch 73/100, Loss: 1401.9231\n",
      "Epoch 74/100, Loss: 1397.7358\n",
      "Epoch 75/100, Loss: 1394.7212\n",
      "Epoch 76/100, Loss: 1390.2716\n",
      "Epoch 77/100, Loss: 1387.1708\n",
      "Epoch 78/100, Loss: 1383.4098\n",
      "Epoch 79/100, Loss: 1380.8245\n",
      "Epoch 80/100, Loss: 1377.0223\n",
      "Epoch 81/100, Loss: 1374.1814\n",
      "Epoch 82/100, Loss: 1370.9225\n",
      "Epoch 83/100, Loss: 1368.6011\n",
      "Epoch 84/100, Loss: 1365.0760\n",
      "Epoch 85/100, Loss: 1363.2394\n",
      "Epoch 86/100, Loss: 1359.7312\n",
      "Epoch 87/100, Loss: 1357.2099\n",
      "Epoch 88/100, Loss: 1354.5790\n",
      "Epoch 89/100, Loss: 1352.0682\n",
      "Epoch 90/100, Loss: 1350.2043\n",
      "Epoch 91/100, Loss: 1347.3877\n",
      "Epoch 92/100, Loss: 1345.6368\n",
      "Epoch 93/100, Loss: 1342.9795\n",
      "Epoch 94/100, Loss: 1340.6846\n",
      "Epoch 95/100, Loss: 1338.0000\n",
      "Epoch 96/100, Loss: 1335.7538\n",
      "Epoch 97/100, Loss: 1334.0380\n",
      "Epoch 98/100, Loss: 1331.9677\n",
      "Epoch 99/100, Loss: 1330.2937\n",
      "Epoch 100/100, Loss: 1328.0533\n",
      "‚úÖ Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "model = NextTokenPredictor(vocab_size, context_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = Y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model, \"cpp_next_token_model2.pt\")\n",
    "print(\"Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59646bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aea9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextTokenPredictor(\n",
       "  (embed): Embedding(43226, 64)\n",
       "  (fc1): Linear(in_features=320, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=43226, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with safe_globals([NextTokenPredictor]):\n",
    "    model = torch.load(\"cpp_next_token_model2.pt\", map_location=device, weights_only=False)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e496ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated continuation:\n",
      "\n",
      "for ( int i = ; i < ; i + + ) { auto negative = json : { json ; std : : vector < llama_token > common_sampler_init ; print_rule ( common_sampler_init , vocab ) / / first_tool_rules to be LLAMA_FTYPE_MOSTLY_Q3_K_M\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_next_tokens(model, seed_tokens, num_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = seed_tokens[:]\n",
    "    for _ in range(num_tokens):\n",
    "        context = generated[-context_size:]\n",
    "        x = torch.tensor([[stoi.get(t, 0) for t in context]], device=device)\n",
    "        logits = model(x) / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs[0], num_samples=1).item()\n",
    "        next_token = itos[next_token_id]\n",
    "        generated.append(next_token)\n",
    "    return \" \".join(generated)\n",
    "\n",
    "# üîπ Try generating\n",
    "# üîπ Ask user for input text\n",
    "user_input = input(\"Enter starting C++ code snippet: \")\n",
    "\n",
    "# üîπ Tokenize the input properly (C++ syntax-aware)\n",
    "user_tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', user_input)\n",
    "\n",
    "# üîπ Handle unknown tokens gracefully\n",
    "user_tokens = [t for t in user_tokens if t in stoi]\n",
    "if len(user_tokens) < context_size:\n",
    "    print(f\"‚ö†Ô∏è Input too short, padding with <unk> tokens\")\n",
    "    user_tokens = ['<unk>'] * (context_size - len(user_tokens)) + user_tokens\n",
    "\n",
    "# üîπ Generate new code continuation\n",
    "generated_code = generate_next_tokens(model, user_tokens, num_tokens=30, temperature=1.0)\n",
    "\n",
    "print(\"\\nGenerated continuation:\\n\")\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
