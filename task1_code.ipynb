{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26f0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.5.0) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d34d02",
   "metadata": {},
   "source": [
    "## Importing the CPP data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef7a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbb3040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e242b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.clang-format',\n",
       " '.clang-tidy',\n",
       " '.devops',\n",
       " '.dockerignore',\n",
       " '.ecrc',\n",
       " '.editorconfig',\n",
       " '.flake8',\n",
       " '.git',\n",
       " '.github',\n",
       " '.gitignore']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check that the repo exists in your current working directory\n",
    "os.listdir(\"llama.cpp\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be902299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 287 .cpp files\n",
      "Example files:\n",
      " ['llama.cpp\\\\common\\\\arg.cpp', 'llama.cpp\\\\common\\\\chat-parser.cpp', 'llama.cpp\\\\common\\\\chat.cpp', 'llama.cpp\\\\common\\\\common.cpp', 'llama.cpp\\\\common\\\\console.cpp', 'llama.cpp\\\\common\\\\json-partial.cpp', 'llama.cpp\\\\common\\\\json-schema-to-grammar.cpp', 'llama.cpp\\\\common\\\\llguidance.cpp', 'llama.cpp\\\\common\\\\log.cpp', 'llama.cpp\\\\common\\\\ngram-cache.cpp']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all .cpp files inside the cloned repo (recursively)\n",
    "cpp_files = glob.glob(\"llama.cpp/**/*.cpp\", recursive=True)\n",
    "\n",
    "print(f\"‚úÖ Found {len(cpp_files)} .cpp files\")\n",
    "print(\"Example files:\\n\", cpp_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c631b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping (no permission): llama.cpp\\tools\\run\\linenoise.cpp\n"
     ]
    }
   ],
   "source": [
    "combined_cpp_text = \"\"\n",
    "\n",
    "for file in cpp_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            combined_cpp_text += f.read() + \"\\n\\n\"\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Skipping (no permission): {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b831613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9625796"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_cpp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72490de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1920541\n",
      "Unique tokens: 43226\n",
      "\n",
      "Top 20 most frequent tokens:\n",
      "\n",
      "token  frequency\n",
      "    ,     158051\n",
      "    )     133160\n",
      "    (     133136\n",
      "    ;      97098\n",
      "    =      75524\n",
      "    .      55274\n",
      "    -      52475\n",
      "    {      48500\n",
      "    }      48314\n",
      "    /      47664\n",
      "    :      45946\n",
      "    *      38311\n",
      "    >      36902\n",
      "    [      27825\n",
      "    ]      27533\n",
      "    <      22894\n",
      "const      20517\n",
      "    +      19943\n",
      "    &      15311\n",
      "   if      11996\n",
      "üìÅ Saved token frequencies to cpp_vocabulary.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ‚úÖ Use your existing in-memory string\n",
    "cpp_text = combined_cpp_text  # already loaded earlier\n",
    "\n",
    "# ‚úÖ Tokenize (C++ syntax-aware)\n",
    "tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', cpp_text)\n",
    "\n",
    "# ‚úÖ Count token frequencies\n",
    "token_counts = Counter(tokens)\n",
    "df_counts = pd.DataFrame(token_counts.items(), columns=[\"token\", \"frequency\"])\n",
    "df_counts = df_counts.sort_values(by=\"frequency\", ascending=False)\n",
    "\n",
    "# ‚úÖ Show stats\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Unique tokens:\", len(df_counts))\n",
    "\n",
    "print(\"\\nTop 20 most frequent tokens:\\n\")\n",
    "print(df_counts.head(20).to_string(index=False))\n",
    "\n",
    "# ‚úÖ Optionally save vocabulary to CSV for later reuse\n",
    "df_counts.to_csv(\"cpp_vocabulary.csv\", index=False)\n",
    "print(\"üìÅ Saved token frequencies to cpp_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8f4d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 1,920,536 training samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ‚úÖ Create mappings (string ‚Üî integer)\n",
    "stoi = {s: i for i, s in enumerate(df_counts[\"token\"])}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "# ‚úÖ Define context size (how many previous tokens to use for prediction)\n",
    "context_size = 5  # you can experiment with 3‚Äì10\n",
    "\n",
    "# ‚úÖ Prepare X (context) and Y (target) lists\n",
    "X, Y = [], []\n",
    "\n",
    "for i in range(len(tokens) - context_size):\n",
    "    context = tokens[i:i + context_size]\n",
    "    target = tokens[i + context_size]\n",
    "    X.append([stoi[t] for t in context])\n",
    "    Y.append(stoi[target])\n",
    "\n",
    "print(f\"‚úÖ Created {len(X):,} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "214bca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1920536, 5])\n",
      "Y shape: torch.Size([1920536])\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Move to torch tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c33187b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71a639d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 8158.2410\n",
      "Epoch 2/100, Loss: 5987.7782\n",
      "Epoch 3/100, Loss: 4995.5158\n",
      "Epoch 4/100, Loss: 4347.9852\n",
      "Epoch 5/100, Loss: 3900.1983\n",
      "Epoch 6/100, Loss: 3581.9716\n",
      "Epoch 7/100, Loss: 3349.8313\n",
      "Epoch 8/100, Loss: 3166.2782\n",
      "Epoch 9/100, Loss: 3023.9326\n",
      "Epoch 10/100, Loss: 2899.5545\n",
      "Epoch 11/100, Loss: 2790.9057\n",
      "Epoch 12/100, Loss: 2696.9978\n",
      "Epoch 13/100, Loss: 2610.9597\n",
      "Epoch 14/100, Loss: 2535.2656\n",
      "Epoch 15/100, Loss: 2465.0388\n",
      "Epoch 16/100, Loss: 2399.4459\n",
      "Epoch 17/100, Loss: 2340.3272\n",
      "Epoch 18/100, Loss: 2286.2762\n",
      "Epoch 19/100, Loss: 2235.7326\n",
      "Epoch 20/100, Loss: 2187.9197\n",
      "Epoch 21/100, Loss: 2144.1381\n",
      "Epoch 22/100, Loss: 2101.4902\n",
      "Epoch 23/100, Loss: 2062.9437\n",
      "Epoch 24/100, Loss: 2026.0425\n",
      "Epoch 25/100, Loss: 1991.7497\n",
      "Epoch 26/100, Loss: 1958.1768\n",
      "Epoch 27/100, Loss: 1927.1082\n",
      "Epoch 28/100, Loss: 1898.3764\n",
      "Epoch 29/100, Loss: 1871.8807\n",
      "Epoch 30/100, Loss: 1845.8584\n",
      "Epoch 31/100, Loss: 1821.3546\n",
      "Epoch 32/100, Loss: 1798.3707\n",
      "Epoch 33/100, Loss: 1776.7507\n",
      "Epoch 34/100, Loss: 1755.7106\n",
      "Epoch 35/100, Loss: 1736.1309\n",
      "Epoch 36/100, Loss: 1718.0862\n",
      "Epoch 37/100, Loss: 1701.0948\n",
      "Epoch 38/100, Loss: 1684.7341\n",
      "Epoch 39/100, Loss: 1668.1499\n",
      "Epoch 40/100, Loss: 1653.3345\n",
      "Epoch 41/100, Loss: 1638.9874\n",
      "Epoch 42/100, Loss: 1625.5924\n",
      "Epoch 43/100, Loss: 1612.6333\n",
      "Epoch 44/100, Loss: 1600.3539\n",
      "Epoch 45/100, Loss: 1588.8766\n",
      "Epoch 46/100, Loss: 1576.8363\n",
      "Epoch 47/100, Loss: 1567.0680\n",
      "Epoch 48/100, Loss: 1557.1219\n",
      "Epoch 49/100, Loss: 1547.7156\n",
      "Epoch 50/100, Loss: 1537.7512\n",
      "Epoch 51/100, Loss: 1530.5579\n",
      "Epoch 52/100, Loss: 1520.7424\n",
      "Epoch 53/100, Loss: 1512.6705\n",
      "Epoch 54/100, Loss: 1505.0107\n",
      "Epoch 55/100, Loss: 1497.5772\n",
      "Epoch 56/100, Loss: 1490.6193\n",
      "Epoch 57/100, Loss: 1483.7742\n",
      "Epoch 58/100, Loss: 1476.9077\n",
      "Epoch 59/100, Loss: 1470.8612\n",
      "Epoch 60/100, Loss: 1464.7945\n",
      "Epoch 61/100, Loss: 1458.6326\n",
      "Epoch 62/100, Loss: 1453.5203\n",
      "Epoch 63/100, Loss: 1448.2998\n",
      "Epoch 64/100, Loss: 1442.3232\n",
      "Epoch 65/100, Loss: 1437.7400\n",
      "Epoch 66/100, Loss: 1432.5934\n",
      "Epoch 67/100, Loss: 1427.5979\n",
      "Epoch 68/100, Loss: 1422.9235\n",
      "Epoch 69/100, Loss: 1418.9552\n",
      "Epoch 70/100, Loss: 1414.1195\n",
      "Epoch 71/100, Loss: 1409.9691\n",
      "Epoch 72/100, Loss: 1405.7970\n",
      "Epoch 73/100, Loss: 1401.9231\n",
      "Epoch 74/100, Loss: 1397.7358\n",
      "Epoch 75/100, Loss: 1394.7212\n",
      "Epoch 76/100, Loss: 1390.2716\n",
      "Epoch 77/100, Loss: 1387.1708\n",
      "Epoch 78/100, Loss: 1383.4098\n",
      "Epoch 79/100, Loss: 1380.8245\n",
      "Epoch 80/100, Loss: 1377.0223\n",
      "Epoch 81/100, Loss: 1374.1814\n",
      "Epoch 82/100, Loss: 1370.9225\n",
      "Epoch 83/100, Loss: 1368.6011\n",
      "Epoch 84/100, Loss: 1365.0760\n",
      "Epoch 85/100, Loss: 1363.2394\n",
      "Epoch 86/100, Loss: 1359.7312\n",
      "Epoch 87/100, Loss: 1357.2099\n",
      "Epoch 88/100, Loss: 1354.5790\n",
      "Epoch 89/100, Loss: 1352.0682\n",
      "Epoch 90/100, Loss: 1350.2043\n",
      "Epoch 91/100, Loss: 1347.3877\n",
      "Epoch 92/100, Loss: 1345.6368\n",
      "Epoch 93/100, Loss: 1342.9795\n",
      "Epoch 94/100, Loss: 1340.6846\n",
      "Epoch 95/100, Loss: 1338.0000\n",
      "Epoch 96/100, Loss: 1335.7538\n",
      "Epoch 97/100, Loss: 1334.0380\n",
      "Epoch 98/100, Loss: 1331.9677\n",
      "Epoch 99/100, Loss: 1330.2937\n",
      "Epoch 100/100, Loss: 1328.0533\n",
      "‚úÖ Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "model = NextTokenPredictor(vocab_size, context_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = Y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model, \"cpp_next_token_model2.pt\")\n",
    "print(\"‚úÖ Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32c5da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 8167.0025\n",
      "Epoch 2/300, Loss: 5997.9117\n",
      "Epoch 3/300, Loss: 5019.7336\n",
      "Epoch 4/300, Loss: 4381.3558\n",
      "Epoch 5/300, Loss: 3941.0862\n",
      "Epoch 6/300, Loss: 3623.0453\n",
      "Epoch 7/300, Loss: 3388.1799\n",
      "Epoch 8/300, Loss: 3213.0073\n",
      "Epoch 9/300, Loss: 3066.4927\n",
      "Epoch 10/300, Loss: 2939.3412\n",
      "Epoch 11/300, Loss: 2829.9130\n",
      "Epoch 12/300, Loss: 2734.5950\n",
      "Epoch 13/300, Loss: 2649.8712\n",
      "Epoch 14/300, Loss: 2572.6255\n",
      "Epoch 15/300, Loss: 2501.6202\n",
      "Epoch 16/300, Loss: 2435.4281\n",
      "Epoch 17/300, Loss: 2374.3439\n",
      "Epoch 18/300, Loss: 2316.9997\n",
      "Epoch 19/300, Loss: 2264.0042\n",
      "Epoch 20/300, Loss: 2213.5781\n",
      "Epoch 21/300, Loss: 2165.9317\n",
      "Epoch 22/300, Loss: 2121.8600\n",
      "Epoch 23/300, Loss: 2080.6596\n",
      "Epoch 24/300, Loss: 2042.0376\n",
      "Epoch 25/300, Loss: 2005.8978\n",
      "Epoch 26/300, Loss: 1972.5151\n",
      "Epoch 27/300, Loss: 1939.7732\n",
      "Epoch 28/300, Loss: 1909.4441\n",
      "Epoch 29/300, Loss: 1881.8013\n",
      "Epoch 30/300, Loss: 1854.7480\n",
      "Epoch 31/300, Loss: 1829.2173\n",
      "Epoch 32/300, Loss: 1805.8855\n",
      "Epoch 33/300, Loss: 1783.1133\n",
      "Epoch 34/300, Loss: 1762.6845\n",
      "Epoch 35/300, Loss: 1742.6342\n",
      "Epoch 36/300, Loss: 1724.0668\n",
      "Epoch 37/300, Loss: 1705.7327\n",
      "Epoch 38/300, Loss: 1688.4003\n",
      "Epoch 39/300, Loss: 1673.3583\n",
      "Epoch 40/300, Loss: 1657.8629\n",
      "Epoch 41/300, Loss: 1642.8534\n",
      "Epoch 42/300, Loss: 1628.7029\n",
      "Epoch 43/300, Loss: 1615.4273\n",
      "Epoch 44/300, Loss: 1602.5828\n",
      "Epoch 45/300, Loss: 1590.9896\n",
      "Epoch 46/300, Loss: 1579.3432\n",
      "Epoch 47/300, Loss: 1567.8646\n",
      "Epoch 48/300, Loss: 1558.7566\n",
      "Epoch 49/300, Loss: 1547.6472\n",
      "Epoch 50/300, Loss: 1538.0434\n",
      "Epoch 51/300, Loss: 1529.5341\n",
      "Epoch 52/300, Loss: 1520.6314\n",
      "Epoch 53/300, Loss: 1512.2257\n",
      "Epoch 54/300, Loss: 1504.1800\n",
      "Epoch 55/300, Loss: 1496.2959\n",
      "Epoch 56/300, Loss: 1489.2440\n",
      "Epoch 57/300, Loss: 1482.0405\n",
      "Epoch 58/300, Loss: 1475.5638\n",
      "Epoch 59/300, Loss: 1469.4729\n",
      "Epoch 60/300, Loss: 1463.5698\n",
      "Epoch 61/300, Loss: 1458.3957\n",
      "Epoch 62/300, Loss: 1451.8135\n",
      "Epoch 63/300, Loss: 1446.4163\n",
      "Epoch 64/300, Loss: 1441.6649\n",
      "Epoch 65/300, Loss: 1436.7372\n",
      "Epoch 66/300, Loss: 1431.7610\n",
      "Epoch 67/300, Loss: 1427.2989\n",
      "Epoch 68/300, Loss: 1422.8478\n",
      "Epoch 69/300, Loss: 1418.8378\n",
      "Epoch 70/300, Loss: 1414.0595\n",
      "Epoch 71/300, Loss: 1409.7802\n",
      "Epoch 72/300, Loss: 1405.7387\n",
      "Epoch 73/300, Loss: 1401.5743\n",
      "Epoch 74/300, Loss: 1398.8489\n",
      "Epoch 75/300, Loss: 1395.0897\n",
      "Epoch 76/300, Loss: 1391.0188\n",
      "Epoch 77/300, Loss: 1387.4704\n",
      "Epoch 78/300, Loss: 1383.9688\n",
      "Epoch 79/300, Loss: 1380.5935\n",
      "Epoch 80/300, Loss: 1378.0199\n",
      "Epoch 81/300, Loss: 1374.1017\n",
      "Epoch 82/300, Loss: 1371.2485\n",
      "Epoch 83/300, Loss: 1368.4154\n",
      "Epoch 84/300, Loss: 1365.2215\n",
      "Epoch 85/300, Loss: 1361.7487\n",
      "Epoch 86/300, Loss: 1359.0182\n",
      "Epoch 87/300, Loss: 1357.3885\n",
      "Epoch 88/300, Loss: 1354.1309\n",
      "Epoch 89/300, Loss: 1350.8438\n",
      "Epoch 90/300, Loss: 1348.5558\n",
      "Epoch 91/300, Loss: 1346.3048\n",
      "Epoch 92/300, Loss: 1344.2077\n",
      "Epoch 93/300, Loss: 1341.7895\n",
      "Epoch 94/300, Loss: 1339.1577\n",
      "Epoch 95/300, Loss: 1336.7998\n",
      "Epoch 96/300, Loss: 1334.4478\n",
      "Epoch 97/300, Loss: 1332.6826\n",
      "Epoch 98/300, Loss: 1330.7263\n",
      "Epoch 99/300, Loss: 1328.2440\n",
      "Epoch 100/300, Loss: 1326.7230\n",
      "Epoch 101/300, Loss: 1325.0729\n",
      "Epoch 102/300, Loss: 1323.4691\n",
      "Epoch 103/300, Loss: 1321.4738\n",
      "Epoch 104/300, Loss: 1320.1394\n",
      "Epoch 105/300, Loss: 1318.6125\n",
      "Epoch 106/300, Loss: 1315.9857\n",
      "Epoch 107/300, Loss: 1314.6489\n",
      "Epoch 108/300, Loss: 1312.4940\n",
      "Epoch 109/300, Loss: 1310.9795\n",
      "Epoch 110/300, Loss: 1309.4874\n",
      "Epoch 111/300, Loss: 1308.1806\n",
      "Epoch 112/300, Loss: 1306.4247\n",
      "Epoch 113/300, Loss: 1304.9597\n",
      "Epoch 114/300, Loss: 1303.2336\n",
      "Epoch 115/300, Loss: 1302.3496\n",
      "Epoch 116/300, Loss: 1300.8649\n",
      "Epoch 117/300, Loss: 1299.3019\n",
      "Epoch 118/300, Loss: 1297.6563\n",
      "Epoch 119/300, Loss: 1296.0902\n",
      "Epoch 120/300, Loss: 1294.7116\n",
      "Epoch 121/300, Loss: 1294.2942\n",
      "Epoch 122/300, Loss: 1292.2848\n",
      "Epoch 123/300, Loss: 1291.4897\n",
      "Epoch 124/300, Loss: 1290.1431\n",
      "Epoch 125/300, Loss: 1288.7144\n",
      "Epoch 126/300, Loss: 1287.7064\n",
      "Epoch 127/300, Loss: 1286.6796\n",
      "Epoch 128/300, Loss: 1286.0435\n",
      "Epoch 129/300, Loss: 1283.9544\n",
      "Epoch 130/300, Loss: 1283.1293\n",
      "Epoch 131/300, Loss: 1282.2202\n",
      "Epoch 132/300, Loss: 1281.4768\n",
      "Epoch 133/300, Loss: 1279.9523\n",
      "Epoch 134/300, Loss: 1279.5180\n",
      "Epoch 135/300, Loss: 1278.5761\n",
      "Epoch 136/300, Loss: 1277.2129\n",
      "Epoch 137/300, Loss: 1277.0936\n",
      "Epoch 138/300, Loss: 1275.8364\n",
      "Epoch 139/300, Loss: 1274.6530\n",
      "Epoch 140/300, Loss: 1274.1977\n",
      "Epoch 141/300, Loss: 1273.0658\n",
      "Epoch 142/300, Loss: 1271.9216\n",
      "Epoch 143/300, Loss: 1271.3005\n",
      "Epoch 144/300, Loss: 1270.8375\n",
      "Epoch 145/300, Loss: 1270.4690\n",
      "Epoch 146/300, Loss: 1269.2868\n",
      "Epoch 147/300, Loss: 1268.5275\n",
      "Epoch 148/300, Loss: 1267.6776\n",
      "Epoch 149/300, Loss: 1266.7743\n",
      "Epoch 150/300, Loss: 1266.1766\n",
      "Epoch 151/300, Loss: 1264.5403\n",
      "Epoch 152/300, Loss: 1264.4578\n",
      "Epoch 153/300, Loss: 1263.3802\n",
      "Epoch 154/300, Loss: 1262.9739\n",
      "Epoch 155/300, Loss: 1261.9765\n",
      "Epoch 156/300, Loss: 1260.7887\n",
      "Epoch 157/300, Loss: 1260.7132\n",
      "Epoch 158/300, Loss: 1260.1680\n",
      "Epoch 159/300, Loss: 1258.6339\n",
      "Epoch 160/300, Loss: 1258.4928\n",
      "Epoch 161/300, Loss: 1257.1648\n",
      "Epoch 162/300, Loss: 1256.7324\n",
      "Epoch 163/300, Loss: 1255.7377\n",
      "Epoch 164/300, Loss: 1254.8965\n",
      "Epoch 165/300, Loss: 1254.8182\n",
      "Epoch 166/300, Loss: 1253.9146\n",
      "Epoch 167/300, Loss: 1252.8555\n",
      "Epoch 168/300, Loss: 1252.4825\n",
      "Epoch 169/300, Loss: 1251.7410\n",
      "Epoch 170/300, Loss: 1251.3808\n",
      "Epoch 171/300, Loss: 1250.4418\n",
      "Epoch 172/300, Loss: 1250.6078\n",
      "Epoch 173/300, Loss: 1248.8238\n",
      "Epoch 174/300, Loss: 1248.9497\n",
      "Epoch 175/300, Loss: 1248.4338\n",
      "Epoch 176/300, Loss: 1248.3797\n",
      "Epoch 177/300, Loss: 1247.4681\n",
      "Epoch 178/300, Loss: 1246.2806\n",
      "Epoch 179/300, Loss: 1247.0780\n",
      "Epoch 180/300, Loss: 1245.8485\n",
      "Epoch 181/300, Loss: 1244.8227\n",
      "Epoch 182/300, Loss: 1244.1688\n",
      "Epoch 183/300, Loss: 1243.9834\n",
      "Epoch 184/300, Loss: 1243.8255\n",
      "Epoch 185/300, Loss: 1242.0221\n",
      "Epoch 186/300, Loss: 1242.2232\n",
      "Epoch 187/300, Loss: 1241.4412\n",
      "Epoch 188/300, Loss: 1241.1069\n",
      "Epoch 189/300, Loss: 1240.2789\n",
      "Epoch 190/300, Loss: 1239.9588\n",
      "Epoch 191/300, Loss: 1239.4848\n",
      "Epoch 192/300, Loss: 1238.8246\n",
      "Epoch 193/300, Loss: 1238.2624\n",
      "Epoch 194/300, Loss: 1237.5600\n",
      "Epoch 195/300, Loss: 1238.0116\n",
      "Epoch 196/300, Loss: 1237.0171\n",
      "Epoch 197/300, Loss: 1236.5678\n",
      "Epoch 198/300, Loss: 1236.4511\n",
      "Epoch 199/300, Loss: 1236.0784\n",
      "Epoch 200/300, Loss: 1235.9314\n",
      "Epoch 201/300, Loss: 1235.8781\n",
      "Epoch 202/300, Loss: 1234.9600\n",
      "Epoch 203/300, Loss: 1234.5005\n",
      "Epoch 204/300, Loss: 1234.0632\n",
      "Epoch 205/300, Loss: 1233.6074\n",
      "Epoch 206/300, Loss: 1233.5033\n",
      "Epoch 207/300, Loss: 1232.9479\n",
      "Epoch 208/300, Loss: 1231.8331\n",
      "Epoch 209/300, Loss: 1231.4870\n",
      "Epoch 210/300, Loss: 1230.8413\n",
      "Epoch 211/300, Loss: 1230.6252\n",
      "Epoch 212/300, Loss: 1230.3946\n",
      "Epoch 213/300, Loss: 1230.1545\n",
      "Epoch 214/300, Loss: 1229.9339\n",
      "Epoch 215/300, Loss: 1229.2199\n",
      "Epoch 216/300, Loss: 1228.7327\n",
      "Epoch 217/300, Loss: 1228.3249\n",
      "Epoch 218/300, Loss: 1228.2340\n",
      "Epoch 219/300, Loss: 1227.7252\n",
      "Epoch 220/300, Loss: 1227.0158\n",
      "Epoch 221/300, Loss: 1226.6234\n",
      "Epoch 222/300, Loss: 1226.7919\n",
      "Epoch 223/300, Loss: 1226.1984\n",
      "Epoch 224/300, Loss: 1225.3757\n",
      "Epoch 225/300, Loss: 1224.8976\n",
      "Epoch 226/300, Loss: 1224.9505\n",
      "Epoch 227/300, Loss: 1224.9535\n",
      "Epoch 228/300, Loss: 1225.2119\n",
      "Epoch 229/300, Loss: 1224.7292\n",
      "Epoch 230/300, Loss: 1223.2131\n",
      "Epoch 231/300, Loss: 1223.5085\n",
      "Epoch 232/300, Loss: 1223.3260\n",
      "Epoch 233/300, Loss: 1222.3384\n",
      "Epoch 234/300, Loss: 1221.8243\n",
      "Epoch 235/300, Loss: 1222.5235\n",
      "Epoch 236/300, Loss: 1221.3801\n",
      "Epoch 237/300, Loss: 1220.9954\n",
      "Epoch 238/300, Loss: 1220.6633\n",
      "Epoch 239/300, Loss: 1220.6406\n",
      "Epoch 240/300, Loss: 1220.2215\n",
      "Epoch 241/300, Loss: 1219.9266\n",
      "Epoch 242/300, Loss: 1219.8412\n",
      "Epoch 243/300, Loss: 1219.0277\n",
      "Epoch 244/300, Loss: 1218.3131\n",
      "Epoch 245/300, Loss: 1218.7071\n",
      "Epoch 246/300, Loss: 1217.5861\n",
      "Epoch 247/300, Loss: 1217.6910\n",
      "Epoch 248/300, Loss: 1217.1678\n",
      "Epoch 249/300, Loss: 1217.6526\n",
      "Epoch 250/300, Loss: 1217.1480\n",
      "Epoch 251/300, Loss: 1216.3608\n",
      "Epoch 252/300, Loss: 1216.4009\n",
      "Epoch 253/300, Loss: 1215.5544\n",
      "Epoch 254/300, Loss: 1215.4521\n",
      "Epoch 255/300, Loss: 1215.1901\n",
      "Epoch 256/300, Loss: 1214.4497\n",
      "Epoch 257/300, Loss: 1214.0266\n",
      "Epoch 258/300, Loss: 1213.8933\n",
      "Epoch 259/300, Loss: 1213.1406\n",
      "Epoch 260/300, Loss: 1213.4336\n",
      "Epoch 261/300, Loss: 1213.0012\n",
      "Epoch 262/300, Loss: 1212.3754\n",
      "Epoch 263/300, Loss: 1212.1026\n",
      "Epoch 264/300, Loss: 1211.9785\n",
      "Epoch 265/300, Loss: 1212.0857\n",
      "Epoch 266/300, Loss: 1211.9627\n",
      "Epoch 267/300, Loss: 1210.9775\n",
      "Epoch 268/300, Loss: 1211.4514\n",
      "Epoch 269/300, Loss: 1210.5102\n",
      "Epoch 270/300, Loss: 1211.0296\n",
      "Epoch 271/300, Loss: 1210.0887\n",
      "Epoch 272/300, Loss: 1209.8238\n",
      "Epoch 273/300, Loss: 1210.0646\n",
      "Epoch 274/300, Loss: 1209.1805\n",
      "Epoch 275/300, Loss: 1209.2689\n",
      "Epoch 276/300, Loss: 1208.7252\n",
      "Epoch 277/300, Loss: 1209.2329\n",
      "Epoch 278/300, Loss: 1208.6435\n",
      "Epoch 279/300, Loss: 1208.0914\n",
      "Epoch 280/300, Loss: 1207.7142\n",
      "Epoch 281/300, Loss: 1207.6738\n",
      "Epoch 282/300, Loss: 1207.4959\n",
      "Epoch 283/300, Loss: 1207.4751\n",
      "Epoch 284/300, Loss: 1206.8063\n",
      "Epoch 285/300, Loss: 1206.6060\n",
      "Epoch 286/300, Loss: 1206.5917\n",
      "Epoch 287/300, Loss: 1206.0448\n",
      "Epoch 288/300, Loss: 1205.7427\n",
      "Epoch 289/300, Loss: 1205.9796\n",
      "Epoch 290/300, Loss: 1205.5081\n",
      "Epoch 291/300, Loss: 1205.5385\n",
      "Epoch 292/300, Loss: 1204.4338\n",
      "Epoch 293/300, Loss: 1205.1147\n",
      "Epoch 294/300, Loss: 1204.6368\n",
      "Epoch 295/300, Loss: 1204.4563\n",
      "Epoch 296/300, Loss: 1203.5329\n",
      "Epoch 297/300, Loss: 1203.5282\n",
      "Epoch 298/300, Loss: 1204.5867\n",
      "Epoch 299/300, Loss: 1204.0404\n",
      "Epoch 300/300, Loss: 1202.8206\n",
      "‚úÖ Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "model = NextTokenPredictor(vocab_size, context_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = Y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model, \"cpp_next_token_model3.pt\")\n",
    "print(\"‚úÖ Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59646bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e496ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated continuation:\n",
      "\n",
      "int a = ; b = a + ; } / / if future , * / , , / / with a having position as the best , void everything , json : : / ) ) ;\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_next_tokens(model, seed_tokens, num_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = seed_tokens[:]\n",
    "    for _ in range(num_tokens):\n",
    "        context = generated[-context_size:]\n",
    "        x = torch.tensor([[stoi.get(t, 0) for t in context]], device=device)\n",
    "        logits = model(x) / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs[0], num_samples=1).item()\n",
    "        next_token = itos[next_token_id]\n",
    "        generated.append(next_token)\n",
    "    return \" \".join(generated)\n",
    "\n",
    "# üîπ Try generating\n",
    "# üîπ Ask user for input text\n",
    "user_input = input(\"Enter starting C++ code snippet: \")\n",
    "\n",
    "# üîπ Tokenize the input properly (C++ syntax-aware)\n",
    "user_tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', user_input)\n",
    "\n",
    "# üîπ Handle unknown tokens gracefully\n",
    "user_tokens = [t for t in user_tokens if t in stoi]\n",
    "if len(user_tokens) < context_size:\n",
    "    print(f\"‚ö†Ô∏è Input too short, padding with <unk> tokens\")\n",
    "    user_tokens = ['<unk>'] * (context_size - len(user_tokens)) + user_tokens\n",
    "\n",
    "# üîπ Generate new code continuation\n",
    "generated_code = generate_next_tokens(model, user_tokens, num_tokens=30, temperature=1.0)\n",
    "\n",
    "print(\"\\nGenerated continuation:\\n\")\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
