{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26f0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.5.0) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d34d02",
   "metadata": {},
   "source": [
    "## Importing the CPP data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef7a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbb3040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e242b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.clang-format',\n",
       " '.clang-tidy',\n",
       " '.devops',\n",
       " '.dockerignore',\n",
       " '.ecrc',\n",
       " '.editorconfig',\n",
       " '.flake8',\n",
       " '.git',\n",
       " '.github',\n",
       " '.gitignore']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check that the repo exists in your current working directory\n",
    "os.listdir(\"llama.cpp\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be902299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 287 .cpp files\n",
      "Example files:\n",
      " ['llama.cpp\\\\common\\\\arg.cpp', 'llama.cpp\\\\common\\\\chat-parser.cpp', 'llama.cpp\\\\common\\\\chat.cpp', 'llama.cpp\\\\common\\\\common.cpp', 'llama.cpp\\\\common\\\\console.cpp', 'llama.cpp\\\\common\\\\json-partial.cpp', 'llama.cpp\\\\common\\\\json-schema-to-grammar.cpp', 'llama.cpp\\\\common\\\\llguidance.cpp', 'llama.cpp\\\\common\\\\log.cpp', 'llama.cpp\\\\common\\\\ngram-cache.cpp']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find all .cpp files inside the cloned repo (recursively)\n",
    "cpp_files = glob.glob(\"llama.cpp/**/*.cpp\", recursive=True)\n",
    "\n",
    "print(f\"‚úÖ Found {len(cpp_files)} .cpp files\")\n",
    "print(\"Example files:\\n\", cpp_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c631b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping (no permission): llama.cpp\\tools\\run\\linenoise.cpp\n"
     ]
    }
   ],
   "source": [
    "combined_cpp_text = \"\"\n",
    "\n",
    "for file in cpp_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            combined_cpp_text += f.read() + \"\\n\\n\"\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Skipping (no permission): {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b831613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9625796"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_cpp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72490de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1920541\n",
      "Unique tokens: 43226\n",
      "\n",
      "Top 20 most frequent tokens:\n",
      "\n",
      "token  frequency\n",
      "    ,     158051\n",
      "    )     133160\n",
      "    (     133136\n",
      "    ;      97098\n",
      "    =      75524\n",
      "    .      55274\n",
      "    -      52475\n",
      "    {      48500\n",
      "    }      48314\n",
      "    /      47664\n",
      "    :      45946\n",
      "    *      38311\n",
      "    >      36902\n",
      "    [      27825\n",
      "    ]      27533\n",
      "    <      22894\n",
      "const      20517\n",
      "    +      19943\n",
      "    &      15311\n",
      "   if      11996\n",
      "üìÅ Saved token frequencies to cpp_vocabulary.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ‚úÖ Use your existing in-memory string\n",
    "cpp_text = combined_cpp_text  # already loaded earlier\n",
    "\n",
    "# ‚úÖ Tokenize (C++ syntax-aware)\n",
    "tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', cpp_text)\n",
    "\n",
    "# ‚úÖ Count token frequencies\n",
    "token_counts = Counter(tokens)\n",
    "df_counts = pd.DataFrame(token_counts.items(), columns=[\"token\", \"frequency\"])\n",
    "df_counts = df_counts.sort_values(by=\"frequency\", ascending=False)\n",
    "\n",
    "# ‚úÖ Show stats\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Unique tokens:\", len(df_counts))\n",
    "\n",
    "print(\"\\nTop 20 most frequent tokens:\\n\")\n",
    "print(df_counts.head(20).to_string(index=False))\n",
    "\n",
    "# ‚úÖ Optionally save vocabulary to CSV for later reuse\n",
    "df_counts.to_csv(\"cpp_vocabulary.csv\", index=False)\n",
    "print(\"üìÅ Saved token frequencies to cpp_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8f4d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 1,920,536 training samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ‚úÖ Create mappings (string ‚Üî integer)\n",
    "stoi = {s: i for i, s in enumerate(df_counts[\"token\"])}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "# ‚úÖ Define context size (how many previous tokens to use for prediction)\n",
    "context_size = 5  # you can experiment with 3‚Äì10\n",
    "\n",
    "# ‚úÖ Prepare X (context) and Y (target) lists\n",
    "X, Y = [], []\n",
    "\n",
    "for i in range(len(tokens) - context_size):\n",
    "    context = tokens[i:i + context_size]\n",
    "    target = tokens[i + context_size]\n",
    "    X.append([stoi[t] for t in context])\n",
    "    Y.append(stoi[target])\n",
    "\n",
    "print(f\"‚úÖ Created {len(X):,} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "214bca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1920536, 5])\n",
      "Y shape: torch.Size([1920536])\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Move to torch tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c33187b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71a639d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 8178.0161\n",
      "Epoch 2/20, Loss: 6007.4413\n",
      "Epoch 3/20, Loss: 5007.2210\n",
      "Epoch 4/20, Loss: 4357.1789\n",
      "Epoch 5/20, Loss: 3903.0950\n",
      "Epoch 6/20, Loss: 3584.7054\n",
      "Epoch 7/20, Loss: 3348.6311\n",
      "Epoch 8/20, Loss: 3168.9155\n",
      "Epoch 9/20, Loss: 3028.4081\n",
      "Epoch 10/20, Loss: 2907.8445\n",
      "Epoch 11/20, Loss: 2802.0616\n",
      "Epoch 12/20, Loss: 2708.4168\n",
      "Epoch 13/20, Loss: 2626.3248\n",
      "Epoch 14/20, Loss: 2550.4460\n",
      "Epoch 15/20, Loss: 2479.0044\n",
      "Epoch 16/20, Loss: 2414.4028\n",
      "Epoch 17/20, Loss: 2354.0456\n",
      "Epoch 18/20, Loss: 2299.0495\n",
      "Epoch 19/20, Loss: 2249.3460\n",
      "Epoch 20/20, Loss: 2202.2419\n",
      "‚úÖ Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "model = NextTokenPredictor(vocab_size, context_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size].to(device)\n",
    "        y_batch = Y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model, \"cpp_next_token_model.pt\")\n",
    "print(\"‚úÖ Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e496ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated continuation:\n",
      "\n",
      "int a = ; b = a + ; } / / if future , * / , , / / with a having position as the best , void everything , json : : / ) ) ;\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_next_tokens(model, seed_tokens, num_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = seed_tokens[:]\n",
    "    for _ in range(num_tokens):\n",
    "        context = generated[-context_size:]\n",
    "        x = torch.tensor([[stoi.get(t, 0) for t in context]], device=device)\n",
    "        logits = model(x) / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs[0], num_samples=1).item()\n",
    "        next_token = itos[next_token_id]\n",
    "        generated.append(next_token)\n",
    "    return \" \".join(generated)\n",
    "\n",
    "# üîπ Try generating\n",
    "# üîπ Ask user for input text\n",
    "user_input = input(\"Enter starting C++ code snippet: \")\n",
    "\n",
    "# üîπ Tokenize the input properly (C++ syntax-aware)\n",
    "user_tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|[{}()\\[\\];.,=+\\-*/<>!&|^%~?:]', user_input)\n",
    "\n",
    "# üîπ Handle unknown tokens gracefully\n",
    "user_tokens = [t for t in user_tokens if t in stoi]\n",
    "if len(user_tokens) < context_size:\n",
    "    print(f\"‚ö†Ô∏è Input too short, padding with <unk> tokens\")\n",
    "    user_tokens = ['<unk>'] * (context_size - len(user_tokens)) + user_tokens\n",
    "\n",
    "# üîπ Generate new code continuation\n",
    "generated_code = generate_next_tokens(model, user_tokens, num_tokens=30, temperature=1.0)\n",
    "\n",
    "print(\"\\nGenerated continuation:\\n\")\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
