{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4374c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed3ce4",
   "metadata": {},
   "source": [
    "## Preprocessing and Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ef5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa938d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e5107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ee90f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mehmetlaudatekman/war-and-peace-project-gutenberg\")\n",
    "file_path = os.path.join(path, \"war_peace_plain.txt\")\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4f4124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\SIDDHESH PATIL\\AppData\\Local\\Temp\\ipykernel_16820\\3697034396.py:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  filtered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of lines:  30660\n",
      "Total unique words:  19764\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "filtered_text = re.sub(r'-', ' ', text)\n",
    "filtered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\n",
    "filtered_text = filtered_text.lower()\n",
    "\n",
    "lines=filtered_text.split(\".\")\n",
    "# text is separated by lines based on full stop.\n",
    "words=['.']\n",
    "for l in lines:\n",
    "    for w in l.split():\n",
    "        if (len(w)>0):\n",
    "            words.append(w)\n",
    "words=list(pd.Series(words).unique())\n",
    "word_counts = Counter(words)\n",
    "\n",
    "df_counts = pd.DataFrame(word_counts.items(), columns = [\"word\", \"frequency\"])\n",
    "df_counts = df_counts.sort_values(by = \"frequency\", ascending=False)  #sorting the words by descending order of their frequency\n",
    "\n",
    "# vocabulary size\n",
    "print(\"Total no. of lines: \", len(lines))\n",
    "print(\"Total unique words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40d8e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent words are: \n",
      "\n",
      "   word  frequency\n",
      "      .          1\n",
      "chapter          1\n",
      "      i          1\n",
      "   well          1\n",
      " prince          1\n",
      "     so          1\n",
      "  genoa          1\n",
      "    and          1\n",
      "  lucca          1\n",
      "    are          1\n",
      "Top 10 least frequent words are: \n",
      "\n",
      "           word  frequency\n",
      "      firmament          1\n",
      "         joshua          1\n",
      "            nun          1\n",
      "      defenders          1\n",
      "      uninvited          1\n",
      "    strengthens          1\n",
      "   immovability          1\n",
      "personalityfree          1\n",
      "         earths          1\n",
      "         unreal          1\n"
     ]
    }
   ],
   "source": [
    "# Top 10 most frequenct words\n",
    "print(\"Top 10 most frequent words are: \\n\")\n",
    "top_10 = df_counts.head(10)\n",
    "print(top_10.to_string(index = False))\n",
    "\n",
    "# Bottom 10 least frequent words\n",
    "print(\"Top 10 least frequent words are: \\n\")\n",
    "bottom_10 = df_counts.tail(10)\n",
    "print(bottom_10.to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb77397",
   "metadata": {},
   "source": [
    "## Model Design and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37822a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping between string and integer to help prediction\n",
    "\n",
    "stoi = {s: i for i,s in enumerate(words)}\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "786c3694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([590609, 5]), torch.Size([590609]), torch.int64, torch.int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 5  # number of previous words used as context\n",
    "X, Y = [], []\n",
    "\n",
    "for line in lines:\n",
    "    predata = [0] * size\n",
    "    prewords = line.split()\n",
    "\n",
    "    for i in range(len(prewords)):\n",
    "        word1 = stoi[prewords[i]]\n",
    "        \n",
    "        X.append(predata.copy())   # store current context\n",
    "        Y.append(word1)         # store next word\n",
    "        \n",
    "        # slide the window\n",
    "        predata = predata[1:] + [word1]\n",
    "\n",
    "        # handle end of sentence\n",
    "        if i == len(prewords) - 1:\n",
    "            eos = stoi['.']\n",
    "            X.append(predata.copy())\n",
    "            Y.append(eos)\n",
    "            predata = predata[1:] + [eos]    # this helps to keep the length of the words needed to be predicted under 5\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X).to(device)\n",
    "Y = torch.tensor(Y).to(device)\n",
    "\n",
    "X.shape, Y.shape, X.dtype, Y.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affce4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(19764, 64)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "# using 64 dimensional embedding \n",
    "\n",
    "embed = torch.nn.Embedding(len(stoi), embed_dim).to(device)\n",
    "# creates embedding layer of 2nd parameter of data from the 1st parameter\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0c3dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Next_Word_Predictor(nn.Module):\n",
    "    def __init__(self, size, vocab_size, embed_dim, hidden_dim, activation_fn, seed_value):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.hyperpams = {'size':self.size, 'embed_dim': embed_dim, 'hidden_dim': hidden_dim, 'activation_fn':activation_fn,'seed_value':seed_value}\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1 = nn.Linear(size* embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.activation_fn = torch.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding Layer\n",
    "        x = self.embed(x)\n",
    "        x = x.view(x.shape[0], -1) # flattens the embedding\n",
    "\n",
    "        # Hidden Layer\n",
    "        x = self.linear1(x)  # maps the flattened vector to the hidden dimension\n",
    "        x = self.activation_fn(x)  # adds an activation function to x\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8051c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, size, embed_dim, vocab_size, hidden_dim, activation_fn, seed_value, device, batch_size=1024, epochs=100, print_every=2):\n",
    "    torch.manual_seed(seed_value)\n",
    "    model = Next_Word_Predictor(size, vocab_size, embed_dim, hidden_dim, activation_fn, seed_value).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            x = X[i:i+batch_size].to(device)\n",
    "            y = Y[i:i+batch_size].to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()  # âœ… fixed\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item()}')\n",
    "\n",
    "    return model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb31d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "hidden_dim = 1024\n",
    "activation_fn = 'relu'\n",
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fefa96d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 6.131535530090332\n",
      "Epoch 2: Loss = 4.148116111755371\n",
      "Epoch 4: Loss = 2.7473084926605225\n",
      "Epoch 6: Loss = 2.2369258403778076\n",
      "Epoch 8: Loss = 1.9346868991851807\n",
      "Epoch 10: Loss = 1.7186203002929688\n",
      "Epoch 12: Loss = 1.546530842781067\n",
      "Epoch 14: Loss = 1.413068175315857\n",
      "Epoch 16: Loss = 1.2983931303024292\n",
      "Epoch 18: Loss = 1.1931016445159912\n",
      "Epoch 20: Loss = 1.107254147529602\n",
      "Epoch 22: Loss = 1.0426493883132935\n",
      "Epoch 24: Loss = 0.9838107824325562\n",
      "Epoch 26: Loss = 0.9329420328140259\n",
      "Epoch 28: Loss = 0.8849213123321533\n",
      "Epoch 30: Loss = 0.8428866267204285\n",
      "Epoch 32: Loss = 0.8042839765548706\n",
      "Epoch 34: Loss = 0.7687357664108276\n",
      "Epoch 36: Loss = 0.7372921705245972\n",
      "Epoch 38: Loss = 0.7091547846794128\n",
      "Epoch 40: Loss = 0.6839790344238281\n",
      "Epoch 42: Loss = 0.661050021648407\n",
      "Epoch 44: Loss = 0.6411907076835632\n",
      "Epoch 46: Loss = 0.6211549043655396\n",
      "Epoch 48: Loss = 0.6035213470458984\n",
      "Epoch 50: Loss = 0.5879759192466736\n",
      "Epoch 52: Loss = 0.5726709961891174\n",
      "Epoch 54: Loss = 0.5587177276611328\n",
      "Epoch 56: Loss = 0.5450829863548279\n",
      "Epoch 58: Loss = 0.5317164063453674\n",
      "Epoch 60: Loss = 0.520058274269104\n",
      "Epoch 62: Loss = 0.5092490315437317\n",
      "Epoch 64: Loss = 0.4986426532268524\n",
      "Epoch 66: Loss = 0.48843562602996826\n",
      "Epoch 68: Loss = 0.4786614775657654\n",
      "Epoch 70: Loss = 0.4712130129337311\n",
      "Epoch 72: Loss = 0.4620307683944702\n",
      "Epoch 74: Loss = 0.4519837498664856\n",
      "Epoch 76: Loss = 0.4472578763961792\n",
      "Epoch 78: Loss = 0.44002363085746765\n",
      "Epoch 80: Loss = 0.4336855709552765\n",
      "Epoch 82: Loss = 0.42760002613067627\n",
      "Epoch 84: Loss = 0.42484596371650696\n",
      "Epoch 86: Loss = 0.42016246914863586\n",
      "Epoch 88: Loss = 0.4181842803955078\n",
      "Epoch 90: Loss = 0.412750244140625\n",
      "Epoch 92: Loss = 0.40890705585479736\n",
      "Epoch 94: Loss = 0.4078857898712158\n",
      "Epoch 96: Loss = 0.4033554196357727\n",
      "Epoch 98: Loss = 0.39990538358688354\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X, Y, size, embed_dim, vocab_size, hidden_dim, activation_fn, seed_value, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a3a1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "torch.save(model, \"model1_task1\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 31,
   "id": "8c2e7b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Next_Word_Predictor(\n",
       "  (embed): Embedding(19764, 64)\n",
       "  (linear1): Linear(in_features=320, out_features=1024, bias=True)\n",
       "  (linear2): Linear(in_features=1024, out_features=19764, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# When loading the model:\n",
    "model = torch.load(\"model1_task1\", map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
>>>>>>> 9631272c7ece5167817645458d8dd6adfd97de8d
   "id": "928e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names generation from the trained model\n",
    "\n",
    "def generateNextWord(model, itos, stoi, content, seed_value, k, temperature =  1, max_len = 10):\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    size = model.size\n",
    "    predata =  content.lower()\n",
    "    predata = re.sub(r'[^a-zA-Z0-9 \\.]', '', predata)  # remove unwanted punctuation\n",
    "    predata = re.sub(r'\\.', ' . ', predata)             # separate periods with spaces\n",
    "\n",
    "\n",
    "    wordsNew = predata.split()\n",
    "    predata = []\n",
    "\n",
    "# Convert words to integer IDs\n",
    "    for i in range(len(wordsNew)):\n",
    "        try:\n",
    "            if stoi[wordsNew[i]]:\n",
    "                predata.append(wordsNew[i])\n",
    "        except:\n",
    "            predata = [stoi[w] for w in predata]\n",
    "            if len(predata) <= size:\n",
    "                predata = [0] * (size - len(predata)) + predata\n",
    "            elif len(predata) > size:\n",
    "                predata = predata[-size:] # take the last (size) elements\n",
    "            x = torch.tensor(predata).view(1, -1).to(device)\n",
    "            y_pred = model(x)\n",
    "            logits = y_pred\n",
    "            logits = logits/temperature\n",
    "\n",
    "            word1 = torch.distributions.categorical.Categorical(logits = logits).sample().item()\n",
    "            word = itos[word1]\n",
    "            content += \" \" + word\n",
    "            predata = predata[1:]+[word1]\n",
    "            predata = [itos[w] for w in predata]\n",
    "\n",
    "\n",
    "    predata = [stoi[w] for w in predata]\n",
    "\n",
    "    if len(predata) <= size:\n",
    "        predata = [0] * (size - len(predata)) + predata\n",
    "    elif len(predata) > size:\n",
    "        predata = predata[-size:]\n",
    "\n",
    "    for i in range(k):\n",
    "        x = torch.tensor(predata).view(1, -1).to(device)\n",
    "        y_pred = model(x)\n",
    "        logits = y_pred\n",
    "        logits = logits/temperature\n",
    "        word1 = torch.distributions.categorical.Categorical(logits=logits).sample().item()\n",
    "        word = itos[word1]\n",
    "        content += \" \" + word\n",
    "        predata = predata[1:] + [word1]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 37,
>>>>>>> 9631272c7ece5167817645458d8dd6adfd97de8d
   "id": "5daaf91f",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19164\\3511978482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the beginning of a sentence: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter number of words to generate: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpar\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgenerateNextWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nGenerated Sentence: \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentence: \n",
      "\n",
      "man is made up of  great importance is\n"
>>>>>>> 9631272c7ece5167817645458d8dd6adfd97de8d
     ]
    }
   ],
   "source": [
    "### Generating next word \n",
    "\n",
    "par = \"\"\n",
    "content = input(\"Enter the beginning of a sentence: \")\n",
    "k = int(input(\"Enter number of words to generate: \"))\n",
    "par += generateNextWord(model, itos, stoi, content, seed_value, k, temperature=1)\n",
    "print(\"\\nGenerated Sentence: \\n\")\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae966a8",
   "metadata": {},
   "source": [
    "## Embedding visualisation and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04e9b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.embed.weight.data.cpu().numpy()\n",
    "# It is used for visualizing the word embeddings using techniques like PCA or t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8171854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "# random module is used to sampling words and TSME is used for reducing dimensions of embeddings for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f88b6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupig the words based on their suffixes\n",
    "def groupWords(itos):\n",
    "    groups = {\n",
    "        'verb_ing' : [],\n",
    "        'verb_ed' : [],\n",
    "        'noun_s' : [],\n",
    "        'adverb_ly' : [],\n",
    "        'adjectives' : [],\n",
    "        'pronouns' : ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them']\n",
    "    }\n",
    "\n",
    "    # dictionary groups with keys for different morphological categories\n",
    "\n",
    "    for word in itos.values():\n",
    "        if word.endswith('ing'):\n",
    "            groups['verb_ing'].append(word)\n",
    "        elif word.endswith('ed'):\n",
    "            groups['verb_ed'].append(word)\n",
    "        elif word.endswith('s'):\n",
    "            groups['noun_s'].append(word)\n",
    "        elif word.endswith('ly'):\n",
    "            groups['adverb_ly'].append(word)\n",
    "        elif word in ['big', 'small', 'happy', 'sad', 'quick', 'slow', 'bright', 'dark']:\n",
    "            groups['adjectives'].append(word)\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62638db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining pronouns\n",
    "pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them']\n",
    "groups = groupWords(itos)\n",
    "groups['pronouns'] = [word for word in pronouns if word in stoi]\n",
    "# Replace the pronouns group with those pronouns that appear in the vocabulary using stoi.\n",
    "\n",
    "# Embedding for all words in the vocabulary\n",
    "\n",
    "allWords = list(stoi.keys())\n",
    "allWordEmbeddings = [model.embed.weight[stoi[word]].detach().cpu().numpy() for word in allWords]\n",
    "allWordEmbeddings = np.array(allWordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fbfc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDDHESH PATIL\\AppData\\Local\\Temp\\ipykernel_16820\\2316895004.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  selected_embeddings = torch.tensor(selected_embeddings)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m perplexity_value = \u001b[38;5;28mmin\u001b[39m(\u001b[32m30\u001b[39m, n_samples - \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m n_samples > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m     22\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=perplexity_value, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m embeddings_tsne = \u001b[43mtsne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallWordEmbeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Mapping from embedding to words\u001b[39;00m\n\u001b[32m     26\u001b[39m word_to_tsne = {word: embed \u001b[38;5;28;01mfor\u001b[39;00m word, embed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(allWords, embeddings_tsne)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1144\u001b[39m, in \u001b[36mTSNE.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[32m   1124\u001b[39m \n\u001b[32m   1125\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1141\u001b[39m \u001b[33;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params_vs_input(X)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_ = embedding\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1034\u001b[39m, in \u001b[36mTSNE._fit\u001b[39m\u001b[34m(self, X, skip_num_points)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[32m   1032\u001b[39m degrees_of_freedom = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_components - \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1086\u001b[39m, in \u001b[36mTSNE._tsne\u001b[39m\u001b[34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[39m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# Learning schedule (part 1): do 250 iteration with lower momentum but\u001b[39;00m\n\u001b[32m   1084\u001b[39m \u001b[38;5;66;03m# higher learning rate controlled via the early exaggeration parameter\u001b[39;00m\n\u001b[32m   1085\u001b[39m P *= \u001b[38;5;28mself\u001b[39m.early_exaggeration\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m params, kl_divergence, it = \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m   1088\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1089\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[t-SNE] KL divergence after \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m iterations with early exaggeration: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m         % (it + \u001b[32m1\u001b[39m, kl_divergence)\n\u001b[32m   1091\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:400\u001b[39m, in \u001b[36m_gradient_descent\u001b[39m\u001b[34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[32m    398\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompute_error\u001b[39m\u001b[33m\"\u001b[39m] = check_convergence \u001b[38;5;129;01mor\u001b[39;00m i == max_iter - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m error, grad = \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m inc = update * grad < \u001b[32m0.0\u001b[39m\n\u001b[32m    403\u001b[39m dec = np.invert(inc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCode\\venv\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:281\u001b[39m, in \u001b[36m_kl_divergence_bh\u001b[39m\u001b[34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[39m\n\u001b[32m    278\u001b[39m indptr = P.indptr.astype(np.int64, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    280\u001b[39m grad = np.zeros(X_embedded.shape, dtype=np.float32)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m error = \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m c = \u001b[32m2.0\u001b[39m * (degrees_of_freedom + \u001b[32m1.0\u001b[39m) / degrees_of_freedom\n\u001b[32m    295\u001b[39m grad = grad.ravel()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# visualising each group\n",
    "\n",
    "for group_name, group in groups.items():\n",
    "    selected_words = random.sample(group, min(50, len(group))) if group else []\n",
    "\n",
    "    # Extract embeddings for the selected words\n",
    "    selected_embeddings = []\n",
    "    selected_word_labels = []\n",
    "\n",
    "    for word in selected_words:\n",
    "        if word in stoi:\n",
    "            index = stoi[word]\n",
    "            selected_embeddings.append(model.embed.weight[index].detach().cpu().numpy())\n",
    "            selected_word_labels.append(word)\n",
    "\n",
    "    selected_embeddings = torch.tensor(selected_embeddings)\n",
    "    n_samples = selected_embeddings.shape[0]\n",
    "    # stores the number of selected vectors\n",
    "\n",
    "    # Reducing the dimensions of embeddings using t-SNE\n",
    "    perplexity_value = min(30, n_samples - 1) if n_samples > 1 else 1\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "    embeddings_tsne = tsne.fit_transform(allWordEmbeddings)\n",
    "\n",
    "    # Mapping from embedding to words\n",
    "    word_to_tsne = {word: embed for word, embed in zip(allWords, embeddings_tsne)}\n",
    "\n",
    "    # Plotting the embeddings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.3, label='All Words')\n",
    "\n",
    "    # Highlight selected words in the group with distinct color\n",
    "    for word in selected_word_labels:\n",
    "        embed = word_to_tsne[word]\n",
    "        plt.scatter(embed[0], embed[1], label=word, color='red', s=100)\n",
    "        plt.annotate(word, (embed[0], embed[1]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "\n",
    "    plt.title(f'Word Embeddings Visualization - Group: {group_name}')\n",
    "    plt.xlabel('TSNE Dimension 1')  \n",
    "    plt.ylabel('TSNE Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb863c2c",
   "metadata": {},
   "source": [
    "Task 1 is completed till visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11672690",
   "metadata": {},
   "source": [
    "Streamlit application update is remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a9c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Example stoi dictionary\n",
    "stoi1 = stoi\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"stoi1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stoi, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c796bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Example stoi dictionary\n",
    "itos1 = itos\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(\"itos1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(itos1, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
